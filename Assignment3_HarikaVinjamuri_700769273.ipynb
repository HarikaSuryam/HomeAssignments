{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNac8X9Oy9NhwkPfovChUPl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarikaSuryam/HomeAssignments/blob/main/Assignment3_HarikaVinjamuri_700769273.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Home Assignment 3 **\n",
        "\n",
        "Student name - Harika Vinjamuri\n",
        "\n",
        "Student Id - 700769273"
      ],
      "metadata": {
        "id": "c-4XfbuKEMgp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "L100shkMVbJw",
        "outputId": "48d9db5e-046b-41ea-8286-b8dc6eb7d8c3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_5 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 582ms/step - loss: 3.6399\n",
            "Epoch 2/5\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 577ms/step - loss: 2.8136\n",
            "Epoch 3/5\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 562ms/step - loss: 2.4286\n",
            "Epoch 4/5\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 559ms/step - loss: 2.2772\n",
            "Epoch 5/5\n",
            "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 554ms/step - loss: 2.1441\n",
            "Generated text:\n",
            "To be or not to be the the nof of Formand the in the enting the the  Entet the the Elet wither alde the the enthe int alle wor to Ik the the Gut in the Woig of or and the the the  Dume and to the the the the the the the Domig of Guing of bith the et an tice in the the the aning the the the the to the the whe the the on the the the the net and sing in Foumed douter to cor the the do tres the wring of to noret on the nont intert in und the Laid fof out as wome the tho the te blet the in the the ep thang couly intion\n"
          ]
        }
      ],
      "source": [
        "# Solution 1\n",
        "\n",
        "# importing required libraries\n",
        "import requests\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Download The Little Prince\n",
        "url = \"https://www.gutenberg.org/files/118/118-0.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "\n",
        "# viewing first 100 chars\n",
        "# print(text[:100])\n",
        "\n",
        "\n",
        "# Unique characters\n",
        "chars = sorted(set(text))\n",
        "# print(f'Total unique chars: {len(chars)}')\n",
        "\n",
        "# craete mapping from char to index and reverse\n",
        "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
        "idx_to_char = {i: ch for ch, i in char_to_idx.items()}\n",
        "\n",
        "# encode text\n",
        "encoded = [char_to_idx[ch] for ch in text]\n",
        "encoded = encoded[:10000] # reduced to improve the performance\n",
        "\n",
        "# training sequences\n",
        "seq_length = 50\n",
        "steps = 1\n",
        "\n",
        "sequences = []\n",
        "next_chars = []\n",
        "\n",
        "for i in range(0, len(encoded) - seq_length, steps):\n",
        "    sequences.append(encoded[i:i+seq_length])\n",
        "    next_chars.append(encoded[i+seq_length])\n",
        "\n",
        "\n",
        "\n",
        "X = np.array(sequences)\n",
        "y = np.array(next_chars)\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(len(chars), 256, input_length=seq_length),\n",
        "    tf.keras.layers.LSTM(256, return_sequences=False),\n",
        "    tf.keras.layers.Dense(len(chars), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "model.summary()\n",
        "\n",
        "\n",
        "epochs = 5\n",
        "batch_size = 128\n",
        "history = model.fit(X, y, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "# Text generation\n",
        "def generate_text(start_string, length=500, temperature=1.0):\n",
        "    \"\"\"Generates text by sampling from the trained LSTM model.\"\"\"\n",
        "    input_indices = [char_to_idx[ch] for ch in start_string]\n",
        "    text = start_string\n",
        "\n",
        "    for _ in range(length):\n",
        "        padded_input = np.pad(input_indices,\n",
        "                                (seq_length - len(input_indices), 0),\n",
        "                                mode='constant', constant_values=0)\n",
        "        padded_input = padded_input.reshape(1, -1)\n",
        "        preds = model.predict(padded_input, verbose=0)[0]\n",
        "\n",
        "        preds /= temperature\n",
        "        preds = np.exp(np.log(preds) / temperature)\n",
        "        preds /= np.sum(preds)\n",
        "\n",
        "        idx = np.random.choice(len(chars), p=preds)\n",
        "        text += idx_to_char[idx]\n",
        "        input_indices.append(idx)\n",
        "        input_indices = input_indices[1:]  # slide forward\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "# Generate 500 chars of new text\n",
        "generated = generate_text(\"To be or not to be \", length=500, temperature=0.5)\n",
        "print(\"Generated text:\")\n",
        "print(generated)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The temperature controls the randomness in sampling the predicted character.\n",
        "\n",
        "Higher temperature (>1): increases randomness; the sampling might produce more unusual or creative text, sometimes less coherent.\n",
        "\n",
        "Lower temperature (<1): makes sampling more deterministic; it sticks close to the most likely character.\n",
        "\n",
        "Temperature == 1: maintains the true predicted probability distribution."
      ],
      "metadata": {
        "id": "XaNUcuyD1eGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution 2\n",
        "\n",
        "# import required libraries\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# defining function\n",
        "def nlp_preprocessing(sentence):\n",
        "    # Tokenizing sentence into words\n",
        "    tokens = word_tokenize(sentence)\n",
        "    print(\"Original Tokens:\")\n",
        "    print(tokens)\n",
        "\n",
        "    # Removing Stopwords\n",
        "    stopwords_set = set(stopwords.words(\"english\"))\n",
        "    tokens_without_stopwords = [word for word in tokens if word.lower() not in stopwords_set]\n",
        "    print(\"\\nTokens Without Stopwords:\")\n",
        "    print(tokens_without_stopwords)\n",
        "\n",
        "    # Perform Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed = [stemmer.stem(word) for word in tokens_without_stopwords]\n",
        "    print(\"\\nStemmed Words:\")\n",
        "    print(stemmed)\n",
        "\n",
        "\n",
        "# apply the defined funaction to the below sentence\n",
        "sentence = \"NLP techniques are used in virtual assistants like Alexa and Siri.\"\n",
        "nlp_preprocessing(sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkJ57j_rqvmh",
        "outputId": "02a2b4a8-86e6-4e7f-b040-f7eea296e46b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tokens:\n",
            "['NLP', 'techniques', 'are', 'used', 'in', 'virtual', 'assistants', 'like', 'Alexa', 'and', 'Siri', '.']\n",
            "\n",
            "Tokens Without Stopwords:\n",
            "['NLP', 'techniques', 'used', 'virtual', 'assistants', 'like', 'Alexa', 'Siri', '.']\n",
            "\n",
            "Stemmed Words:\n",
            "['nlp', 'techniqu', 'use', 'virtual', 'assist', 'like', 'alexa', 'siri', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stemming:**\n",
        "Stemming cuts words down to their base form by removing suffixes.\n",
        "Example: stemming “running” -> “run”\n",
        "\n",
        "**Lemmatization:**\n",
        "Lemmatization converts words to their base or dictionary form (lemma) while considering context and POS (part of speech).\n",
        "Example: lemmatization “running” -> “run”"
      ],
      "metadata": {
        "id": "cvCCfJpivA8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove stop words to reduce noise and improve algorithm performance in tasks like document clustering, information retrieval, or topic modeling — where high-frequency words carry little semantic weight.\n",
        "\n",
        "Stop words can be crucial for understanding context and meaning, especially in sentiment analysis, dependency parsing, or machine translation, where their presence or absence can affect semantic relationships."
      ],
      "metadata": {
        "id": "NBeVwu0Bvg3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution 3\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load english language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# input\n",
        "sentence = \"Barack Obama served as the 44th President of the United States and won the Nobel Peace Prize in 2009.\"\n",
        "\n",
        "# Process sentence with spaCy\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Printing extracted entities\n",
        "for ent in doc.ents:\n",
        "    print(f\"Text: {ent.text}, Label: {ent.label_}, Start: {ent.start_char}, End: {ent.end_char}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yBCNbo1vimD",
        "outputId": "c7f94347-45d3-4f6d-bb7c-400e4a8f5e89"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: Barack Obama, Label: PERSON, Start: 0, End: 12\n",
            "Text: 44th, Label: ORDINAL, Start: 27, End: 31\n",
            "Text: the United States, Label: GPE, Start: 45, End: 62\n",
            "Text: the Nobel Peace Prize, Label: WORK_OF_ART, Start: 71, End: 92\n",
            "Text: 2009, Label: DATE, Start: 96, End: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NER (Named Entity Recognition)** focuses on identifying entities in the text, such as people, organizations, locations, dates, and more.\n",
        "\n",
        "**POS (Part of Speech)** tagging, meanwhile, focuses on grammatical roles (noun, verb, adjectival, etc.).\n"
      ],
      "metadata": {
        "id": "3WpEwnhq-X5E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Real Time Applications for NER :**\n",
        "\n",
        "Financial News: NER helps identify companies, people, and financial instruments in financial articles, which can aid in financial analysis and automated trading decisions.\n",
        "\n",
        "Search Engines: NER helps search engines identify entities within search queries to provide more accurate and context-specific search results (for example, distinguishing “Amazon” the company from “Amazon” the rain forest)."
      ],
      "metadata": {
        "id": "gjQWHujD-iME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution 4\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V):\n",
        "    \"\"\"\n",
        "    Perform scaled dot-product attention.\n",
        "\n",
        "    Args:\n",
        "        Q: Query matrix\n",
        "        K: Key matrix\n",
        "        V: Value matrix\n",
        "\n",
        "    Returns:\n",
        "        Attention weights and output after applying attention\n",
        "    \"\"\"\n",
        "    d = K.shape[1]  # dimension of key\n",
        "\n",
        "    # Calculating raw scores\n",
        "    scores = np.dot(Q, K.T).astype(float)\n",
        "\n",
        "    # Square root of d\n",
        "    scores /= np.sqrt(d)\n",
        "\n",
        "    # Applying softmax\n",
        "    weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n",
        "    weights /= np.sum(weights, axis=-1, keepdims=True)\n",
        "\n",
        "    # Multiply weights\n",
        "    output = np.dot(weights, V)\n",
        "    return weights, output\n",
        "\n",
        "\n",
        "# inputs\n",
        "Q = np.array([\n",
        "    [1, 0, 1, 0],\n",
        "    [0, 1, 0, 1]\n",
        "], dtype=float)\n",
        "\n",
        "K = np.array([\n",
        "    [1, 0, 1, 0],\n",
        "    [0, 1, 0, 1]\n",
        "], dtype=float)\n",
        "\n",
        "V = np.array([\n",
        "    [1, 2, 3, 4],\n",
        "    [5, 6, 7, 8]\n",
        "], dtype=float)\n",
        "\n",
        "\n",
        "# Scaled dot-product\n",
        "weights, output = scaled_dot_product_attention(Q, K, V)\n",
        "\n",
        "print(\"Attention Weights:\")\n",
        "print(weights)\n",
        "print(\"\\nFinal Output:\")\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiRXKQclwcIb",
        "outputId": "fa6ed6c0-9997-41cf-bbdd-7e6326828fa7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights:\n",
            "[[0.73105858 0.26894142]\n",
            " [0.26894142 0.73105858]]\n",
            "\n",
            "Final Output:\n",
            "[[2.07576569 3.07576569 4.07576569 5.07576569]\n",
            " [3.92423431 4.92423431 5.92423431 6.92423431]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Division performed for attention score by underroot d in the scaled dot-product:**\n",
        "\n",
        "We scale by the square root of the key dimension (d) to avoid the dot product growing too large with high dimensions. Large scores can push the softmax into saturated regimes, which can diminish the gradient and make training less effective. So scaling helps keep the values within a range where softmax operates gracefully."
      ],
      "metadata": {
        "id": "hK8gnPEEAJyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Self-attention** lets each word attend to all other words in the sentence. This way, the model can learn dependency relationships, context, and semantic connections, regardless of their positions in the sentence. This ability to combine information from the entire sentence is a key advantage for understanding context and generating coherent representations."
      ],
      "metadata": {
        "id": "Ob8tVM4FAuB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZYVZ_9NBKhH",
        "outputId": "57dc8d47-ebb0-4615-d584-aa0a561caa36"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution 4\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load sentiment analysis pipeline\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model='distilbert-base-uncased-finetuned-sst-2-english')\n",
        "\n",
        "# Input sentence\n",
        "sentence = \"Despite the high price, the performance of the new MacBook is outstanding.\"\n",
        "\n",
        "# Perform sentiment analysis\n",
        "result = sentiment_pipeline(sentence)[0]\n",
        "\n",
        "# Printing label and confidence score\n",
        "print(f\"Sentiment: {result['label']}\")\n",
        "print(f\"Confidence Score: {result['score']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYoKVv4s_lFO",
        "outputId": "7284caf2-f79b-44fb-8485-cc67f11e2032"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: POSITIVE\n",
            "Confidence Score: 0.9998302459716797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BERT (Bidirectional Encoder Representations from Transformers)** uses an encoder architecture, which focuses on understanding context by looking at both the left and the right side of a word.\n",
        "\n",
        "**GPT (Generative Pre-trained Transformer)** uses a decoder architecture, which generates text by looking backwards (typically left-to-right)."
      ],
      "metadata": {
        "id": "Ofn7qd6zC54b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using pre-trained models is beneficial because:**\n",
        "\n",
        "General Knowledge: They already learn rich linguistic patterns, semantic relationships, and context from vast amounts of text.\n",
        "\n",
        "Training Time and Resources: It saves significant computational resources and training time.\n",
        "\n",
        "Higher Performance with Less Data: Pre-trained models enable fine-tuning on small, domain-specific datasets while retaining their strong general understanding of language."
      ],
      "metadata": {
        "id": "zDOc8bgRDKbz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sphgIT4BBaWK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}